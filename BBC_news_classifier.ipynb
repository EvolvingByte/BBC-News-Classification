{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_selection import chi2,SelectKBest\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Linear SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "#Cross-Validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feaf2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting data path\n",
    "data_path = \"./datasets_coursework1/bbc/\"\n",
    "os.listdir(data_path) #listing content inside path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting folders into a list\n",
    "folders = [f for f in os.listdir(data_path) if f not in [\"README.TXT\", \"bbc.csv\"]]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One by one reading and retrieving data from data path\n",
    "news = []\n",
    "category = []\n",
    "for folder in folders:\n",
    "    internal_path = data_path + folder\n",
    "    files = os.listdir(internal_path)\n",
    "    for t_files in files:\n",
    "        t_path = internal_path + '/' + t_files\n",
    "        with open(t_path, 'r') as f:\n",
    "            content = f.readlines()\n",
    "        content = ' '.join(content)\n",
    "        news.append(content)\n",
    "        category.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3608be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdict = {'News' :news, 'Category': category} #setting dictionary to transform data into data frame\n",
    "df = pd.DataFrame(tempdict) #creating data frame\n",
    "df.to_csv(\"./datasets_coursework1/bbc.csv\") #saving data to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced454e",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer() #initializing WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6741e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "processed_text = []\n",
    "new_text = \" \"\n",
    "for n in range(len(df.News)):\n",
    "    new_text = re.sub(r\"\\W\", \" \", str(df.News[n])) #Replacing non-word characters with spaces\n",
    "    new_text = new_text.lower() #Coverting corpus to lower-case\n",
    "    new_text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", new_text) #Replacing single characters with spaces\n",
    "    new_text = re.sub(r\"\\s+\", \" \", new_text) #Removing extra spaces\n",
    "    processed_text.append(new_text) #Getting pre-processed data\n",
    "\n",
    "processed = map(lambda x:' '.join([lem.lemmatize(word) for word in x.split()]), processed_text) #mapping lemmatized data\n",
    "processed_text = list(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b75ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\") #Getting English Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb119e",
   "metadata": {},
   "source": [
    "## Bag of Words Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce827424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words\n",
    "\n",
    "count = CountVectorizer(min_df = 5, max_df=0.6, stop_words=stopwords)\n",
    "edit_text_1 = count.fit_transform(processed_text).toarray() #ndarray of bag of words\n",
    "edit_text_1 = SelectKBest(chi2, k=1500).fit_transform(edit_text_1,df.Category) #1500 relevent features are selected\n",
    "edit_text_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd37621",
   "metadata": {},
   "source": [
    "## Unigram TF-IDF Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF model\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3, stop_words=stopwords, norm='l2', ngram_range=(1,1))\n",
    "edit_text_2 = tfidf.fit_transform(processed_text).toarray() #ndarray of TF-IDF unigrams\n",
    "edit_text_2 = SelectKBest(chi2, k=1500).fit_transform(edit_text_2,df.Category) ##1500 relevent features are selected\n",
    "edit_text_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261218a",
   "metadata": {},
   "source": [
    "## Bi-gram TF-IDF Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86637367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bi-gram model\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3, stop_words=stopwords, norm='l2', ngram_range=(2,2))\n",
    "edit_text_3 = tfidf.fit_transform(processed_text).toarray() #ndarray of TF-IDF bigrams\n",
    "edit_text_3 = SelectKBest(chi2, k=1500).fit_transform(edit_text_3,df.Category) #1500 relevent features are selected\n",
    "edit_text_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d28f1",
   "metadata": {},
   "source": [
    "## Feature Stacking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b838ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature horizontal stacking\n",
    "edit_text = np.hstack((edit_text_1, edit_text_2, edit_text_3))\n",
    "edit_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f15c2",
   "metadata": {},
   "source": [
    "## Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get Accuracy, F1-score, Precision, Recall\n",
    "def get_scores_of(model, X_tr, X_te, y_tr, y_te):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te) \n",
    "    Acc = metrics.accuracy_score(y_te,y_pred)\n",
    "    F1 = metrics.f1_score(y_te,y_pred,average='macro')\n",
    "    Pre = metrics.precision_score(y_te,y_pred, average='macro')\n",
    "    Rec = metrics.recall_score(y_te,y_pred, average='macro')\n",
    "    return Acc, F1, Pre, Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "K = StratifiedKFold(n_splits=10) #Stratified K-Fold cross-validation\n",
    "ModelScoresAcc, ModelScoresF1, ModelScoresPre, ModelScoresRec = [],[],[],[]\n",
    "\n",
    "for train_i, test_i in K.split(edit_text,category):\n",
    "    X_train, X_test, y_train, y_test = edit_text[train_i], edit_text[test_i], df.Category[train_i], df.Category[test_i]\n",
    "    acc, f1, pre, rec = get_scores_of(LinearSVC(max_iter=6000, multi_class='ovr'),X_train, X_test, y_train, y_test)\n",
    "    ModelScoresAcc.append(acc)\n",
    "    ModelScoresF1.append(f1)\n",
    "    ModelScoresPre.append(pre)\n",
    "    ModelScoresRec.append(rec)\n",
    "\n",
    "print(\"With LinearSVC:\")\n",
    "print(\"Accuracy of the model: {:.2f}\".format(float(np.mean(ModelScoresAcc)*100)), \"%\")\n",
    "print(\"Macro averaged F1 score of the model: {:.2f}\".format(float(np.mean(ModelScoresF1)*100)), \"%\")\n",
    "print(\"Macro averaged precision of the model: {:.2f}\".format(float(np.mean(ModelScoresPre)*100)), \"%\")\n",
    "print(\"Macro averaged Recall of the model: {:.2f}\".format(float(np.mean(ModelScoresRec)*100)), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28803aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
